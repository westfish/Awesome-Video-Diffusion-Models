# Awesome-Video-Diffusion-Models


[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)  ![GitHub stars](https://img.shields.io/github/stars/westfish/Awesome-NLP-Diffusion-Models?color=green)  ![GitHub forks](https://img.shields.io/github/forks/westfish/Awesome-NLP-Diffusion-Models?color=9cf)

A collection of resources and papers on ***Diffusion Models of Video Generation***.


## Contents

- [Awesome-Video-Diffusion-Models](#awesome-video-diffusion-models)
  - [Contents](#contents)
  - [Papers](#papers)
  - [Tutorials](#tutorials)
  - [Blogs](#blogs)

## Papers
|Link|Year|Conference|Paper|First Author and Institute|
| :- | :- | :- | :- | :- |
|<p>[paper](https://arxiv.org/pdf/2302.01329v1.pdf)</p><p>[website](https://dreamix-video-editing.github.io/)</p>|2023/02|arxiv|Dreamix: Video Diffusion Models are General Video Editors|<p>Eyal Molad</p><p> </p><p>Google Research</p>|
|<p>[paper](https://tuneavideo.github.io/static/paper/tune-a-video.pdf)</p><p>[paper2](https://arxiv.org/pdf/2212.11565.pdf)</p><p>[website](https://tuneavideo.github.io/)</p><p>[github](https://github.com/showlab/Tune-A-Video)</p><p> </p>|2022/12|arxiv|Tune-A-Video: One-Shot Tuning of Image Diffusion Models|<p>Jay Zhangjie Wu</p><p> </p><p>National University of Singapore</p>|
|<p>[paper](https://arxiv.org/pdf/2212.00235.pdf)</p><p>[website](https://kfmei.page/vidm/)</p><p>[github](https://github.com/MKFMIKU/VIDM)</p>|2022/12|<p>arxiv</p><p>AAAI-2023</p>|VIDM: Video Implicit Diffusion Models|<p>Kangfu Mei</p><p> </p><p>Johns Hopkins University</p>|
|<p>[paper](https://arxiv.org/pdf/2211.11743.pdf)</p><p>[website](https://yanivnik.github.io/sinfusion/)</p>|2022/11|arxiv|Sinfusion: Training diffusion models on a single image or video|<p>Yaniv Nikankin, Niv Haim</p><p> </p><p>Weizmann Institute of Science</p>|
|<p>[paper](https://arxiv.org/pdf/2211.11018.pdf)</p><p>[website](https://magicvideo.github.io/)</p>|2022/11|arxiv|MagicVideo: Efficient Video Generation With Latent Diffusion Models|<p>Daquan Zhou, Weimin Wang</p><p> </p><p>ByteDance Inc</p>|
|<p>[paper](https://imagen.research.google/video/paper.pdf)</p><p>[paper2](https://arxiv.org/pdf/2210.02303.pdf)</p><p> </p>|2022/10|arxiv|IMAGEN VIDEO: HIGH DEFINITION VIDEO GENERATION WITH DIFFUSION MODELS|<p>Jonathan Ho</p><p> </p><p>Google</p>|
|<p>[paper](https://arxiv.org/abs/2209.14792)</p><p>[website](https://makeavideo.studio/)</p><p> </p>|2022/09|arxiv|MAKE-A-VIDEO: TEXT-TO-VIDEO GENERATION WITHOUT TEXT-VIDEO DATA|<p>Uriel Singer</p><p> </p><p>Meta</p>|
|<p>[paper](https://arxiv.org/pdf/2204.03458.pdf)</p><p>[website](https://video-diffusion.github.io/)</p><p> </p>|2022/04|<p>arxiv</p><p>NeurIps 2022</p>|Video Diffusion Models|<p>Jonathan Ho</p><p> </p><p>Google</p>|






## Tutorials



## Blogs

- What are Diffusion Models? _Lilian Weng_. [Website](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)